
@Inbook{dale_2016,
  author="Dale, Matthew
  and Miller, Julian F.
  and Stepney, Susan",
  title={Reservoir computing as a model for in-materio computing},
  bookTitle="Advances in Unconventional Computing: Volume 1: Theory",
  year="2017",
  publisher="Springer International Publishing",
  address="Cham",
  pages="533--571",
  abstract="Research in substrate-based computing has shown that materials contain rich properties that can be exploited to solve computational problems. One such technique known as Evolution-in-materio uses evolutionary algorithms to manipulate material substrates for computation. However, in general, modelling the computational processes occurring in such systems is a difficult task and understanding what part of the embodied system is doing the computation is still fairly ill-defined. This chapter discusses the prospects of using Reservoir Computing as a model for in-materio computing, introducing new training techniques (taken from Reservoir Computing) that could overcome training difficulties found in the current Evolution-in-Materio technique.",
  isbn="978-3-319-33924-5",
  doi="10.1007/978-3-319-33924-5_22",
  url="https://doi.org/10.1007/978-3-319-33924-5_22"
}


@InProceedings{fernando_and_sojakka_2003,
  author="Fernando, Chrisantha
  and Sojakka, Sampsa",
  editor="Banzhaf, Wolfgang
  and Ziegler, Jens
  and Christaller, Thomas
  and Dittrich, Peter
  and Kim, Jan T.",
  title={Pattern recognition in a bucket},
  booktitle="Advances in Artificial Life",
  year="2003",
  publisher="Springer Berlin Heidelberg",
  address="Berlin, Heidelberg",
  pages="588--597",
  abstract="This paper demonstrates that the waves produced on the surface of water can be used as the medium for a ``Liquid State Machine'' that pre-processes inputs so allowing a simple perceptron to solve the XOR problem and undertake speech recognition. Interference between waves allows non-linear parallel computation upon simultaneous sensory inputs. Temporal patterns of stimulation are converted to spatial patterns of water waves upon which a linear discrimination can be made. Whereas Wolfgang Maass' Liquid State Machine requires fine tuning of the spiking neural network parameters, water has inherent self-organising properties such as strong local interactions, time-dependent spread of activation to distant areas, inherent stability to a wide variety of inputs, and high complexity. Water achieves this ``for free'', and does so without the time-consuming computation required by realistic neural models. An analogy is made between water molecules and neurons in a recurrent neural network.",
  isbn="978-3-540-39432-7"
}

@techreport{jaeger_2001,
  author       = {Herbert Jaeger},
  title        = {The ``echo state'' approach to analysing and training recurrent neural networks},
  type         = {GMD Report},
  number       = {148},
  institution  = {German National Research Center for Information Technology},
  address      = {Sankt Augustin, Germany},
  year         = {2001},
  url          = {https://www.ai.rug.nl/minds/uploads/EchoStatesTechRep.pdf},
  note         = {With an erratum note}
}


@article{lukosevicius_and_jaeger_2009,
  title = {Reservoir computing approaches to recurrent neural network training},
  journal = {Computer Science Review},
  volume = {3},
  number = {3},
  pages={127--149},
  year = {2009},
  issn = {1574-0137},
  doi = {https://doi.org/10.1016/j.cosrev.2009.03.005},
  url = {https://www.sciencedirect.com/science/article/pii/S1574013709000173},
  author = {Mantas Lukoševičius and Herbert Jaeger},
  abstract = {Echo State Networks and Liquid State Machines introduced a new paradigm in artificial recurrent neural network (RNN) training, where an RNN (the reservoir) is generated randomly and only a readout is trained. The paradigm, becoming known as reservoir computing, greatly facilitated the practical application of RNNs and outperformed classical fully trained RNNs in many tasks. It has lately become a vivid research field with numerous extensions of the basic idea, including reservoir adaptation, thus broadening the initial paradigm to using different methods for training the reservoir and the readout. This review systematically surveys both current ways of generating/adapting the reservoirs and training different types of readouts. It offers a natural conceptual classification of the techniques, which transcends boundaries of the current “brand-names” of reservoir methods, and thus aims to help in unifying the field and providing the reader with a detailed “map” of it.}
}

@ARTICLE{buehner_young_2006,
  author={Buehner, M. and Young, P.},
  journal={IEEE Transactions on Neural Networks}, 
  title={A tighter bound for the echo state property}, 
  year={2006},
  volume={17},
  number={3},
  pages={820--824},
  keywords={Recurrent neural networks;Nonlinear systems;Sufficient conditions;Control system synthesis;Nonlinear control systems;Nonlinear dynamical systems;Robust control;Artificial neural networks;Asymptotic stability;Lyapunov method;Echo state networks (ESNs);Lyapunov stability;nonlinear systems;recurrent neural networks (RNN);robust controls;weighted operator norms},
  doi={10.1109/TNN.2006.872357}
}

@ARTICLE{jacobs_1991,
  author={Jacobs, Robert A. and Jordan, Michael I. and Nowlan, Steven J. and Hinton, Geoffrey E.},
  journal={Neural Computation}, 
  title={Adaptive mixtures of local experts}, 
  year={1991},
  volume={3},
  number={1},
  pages={79--87},
  keywords={},
  doi={10.1162/neco.1991.3.1.79}
}

@ARTICLE{jordan_1994,
  author={Jordan, Michael I. and Jacobs, Robert A.},
  journal={Neural Computation}, 
  title={Hierarchical mixtures of experts and the EM algorithm}, 
  year={1994},
  volume={6},
  number={2},
  pages={181--214},
  keywords={},
  doi={10.1162/neco.1994.6.2.181}
}


@inproceedings{tabuse_kinouchi_and_hagiwara_1997,
  author={Tabuse, M. and Kinouchi, M. and Hagiwara, M.},
  booktitle={1997 IEEE International Conference on Systems, Man, and Cybernetics. Computational Cybernetics and Simulation}, 
  title={Recurrent neural network using mixture of experts for time series processing}, 
  year={1997},
  volume={1},
  number={},
  pages={536--541 vol.1},
  keywords={Recurrent neural networks;Delay effects;Biological neural networks;Convergence;Neurofeedback;Jacobian matrices;Electronic mail;Computer networks;Speech processing;Motion pictures},
  doi={10.1109/ICSMC.1997.625807}
}



@article{shazeer_2017,
  author       = {Noam Shazeer and
                  Azalia Mirhoseini and
                  Krzysztof Maziarz and
                  Andy Davis and
                  Quoc V. Le and
                  Geoffrey E. Hinton and
                  Jeff Dean},
  title        = {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts
                  Layer},
  journal      = {CoRR},
  volume       = {abs/1701.06538},
  year         = {2017},
  url          = {http://arxiv.org/abs/1701.06538},
  eprinttype    = {arXiv},
  eprint       = {1701.06538},
  timestamp    = {Mon, 13 Aug 2018 16:46:11 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/ShazeerMMDLHD17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@InProceedings{babinec_pospichal_2009,
author="Babinec, {\v{S}}tefan
and Posp{\'i}chal, Ji{\v{r}}{\'i}",
editor="K{\"o}ppen, Mario
and Kasabov, Nikola
and Coghill, George",
title={Gating echo state neural networks for time series forecasting},
booktitle="Advances in Neuro-Information Processing",
year="2009",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="200--207",
abstract="``Echo State'' neural networks, which are a special case of recurrent neural networks, are studied from the viewpoint of their learning ability, with a goal to achieve their greater predictive ability. In this paper we study the influence of the memory length on predictive abilities of Echo State neural networks. The conclusion is that Echo State neural networks with fixed memory length can have troubles with adaptation of its intrinsic dynamics to dynamics of the prediction task. Therefore, we have tried to create complex prediction system as a combination of the local expert Echo State neural networks with different memory length and one special gating Echo State neural network. This approach was tested in laser fluctuations prediction. The prediction error achieved by this approach was substantially smaller in comparison with prediction error achieved by standard Echo State neural networks.",
isbn="978-3-642-02490-0"
}


@techreport{jaeger_2007,
  author={Jaeger, Herbert},
  title     = {Discovering multiscale dynamical features with hierarchical echo state networks},
  series    = {Constructor University Technical Reports},
  number    = {10},
  year        = {2007},
  institution={Jacobs University Bremen}
}


@article{ma_2021,
  title = {Deepr-esn: A deep projection-encoding echo-state network},
  journal = {Information Sciences},
  volume = {511},
  pages={152--171},
  year = {2020},
  issn = {0020-0255},
  doi = {https://doi.org/10.1016/j.ins.2019.09.049},
  url = {https://www.sciencedirect.com/science/article/pii/S0020025519309053},
  author = {Qianli Ma and Lifeng Shen and Garrison W. Cottrell},
  keywords = {Hierarchical reservoir computing, Echo state network, Multiscale dynamics, Time series prediction},
  abstract = {As a recurrent neural network that requires no training, the reservoir computing (RC) model has attracted widespread attention in the last decade, especially in the context of time series prediction. However, most time series have a multiscale structure, which a single-hidden-layer RC model may have difficulty capturing. In this paper, we propose a novel multiple projection-encoding hierarchical reservoir computing framework called Deep Projection-encoding Echo State Network (DeePr-ESN). The most distinctive feature of our model is its ability to learn multiscale dynamics through stacked ESNs, connected via subspace projections. Specifically, when an input time series is projected into the high-dimensional echo-state space of a reservoir, a subsequent encoding layer (e.g., an autoencoder or PCA) projects the echo-state representations into a lower-dimensional feature space. These representations are the principal components of the echo-state representations, which removes the high frequency components of the representations. These can then be processed by another ESN through random connections. By using projection layers and encoding layers alternately, our DeePr-ESN can provide much more robust generalization performance than previous methods, and also fully takes advantage of the temporal kernel property of ESNs to encode the multiscale dynamics of time series. In our experiments, the DeePr-ESNs outperform both standard ESNs and existing hierarchical reservoir computing models on some artificial and real-world time series prediction tasks.}
}

editor="Montavon, Gr{\'e}goire
  and Orr, Genevi{\`e}ve B.
  and M{\"u}ller, Klaus-Robert",
@Inbook{lukosevicius_2012_practical_guide,
  author="Luko{\v{s}}evi{\v{c}}ius, Mantas",
  title={A practical guide to applying echo state networks},
  bookTitle="Neural Networks: Tricks of the Trade: Second Edition",
  year="2012",
  publisher="Springer Berlin Heidelberg",
  address="Berlin, Heidelberg",
  pages="659--686",
  abstract="Reservoir computing has emerged in the last decade as an alternative to gradient descent methods for training recurrent neural networks. Echo State Network (ESN) is one of the key reservoir computing ``flavors''. While being practical, conceptually simple, and easy to implement, ESNs require some experience and insight to achieve the hailed good performance in many tasks. Here we present practical techniques and recommendations for successfully applying ESNs, as well as some more advanced application-specific modifications.",
  isbn="978-3-642-35289-8",
  doi="10.1007/978-3-642-35289-8_36",
  url="https://doi.org/10.1007/978-3-642-35289-8_36"
}

@InProceedings{wringe_2024,
  author="Wringe, Chester
  and Stepney, Susan
  and Trefzer, Martin A.",
  editor="Wand, Michael
  and Malinovsk{\'a}, Krist{\'i}na
  and Schmidhuber, J{\"u}rgen
  and Tetko, Igor V.",
  title={Restricted reservoirs on heterogeneous timescales},
  booktitle="Artificial Neural Networks and Machine Learning -- ICANN 2024",
  year="2024",
  publisher="Springer Nature Switzerland",
  address="Cham",
  pages="168--183",
  abstract="The Reservoir Computing model is one that is suited to computing with physical materials. However, the limitations of those materials can lead to low computational power. To address this, we plan to combine reservoirs operating on different timescales together to create a heterogeneous reservoir. We simulate this using a new multiple timescale ESN model. We also introduce ``mock materials'' so that future works may focus on combining different materials to study the effect. We benchmark our multi-timescale ESN on a multiple timescale problem, MSO.",
  isbn="978-3-031-72359-9"
}

@ARTICLE{deng_and_zhang_2007,
  author={Deng, Zhidong and Zhang, Yi},
  journal={IEEE Transactions on Neural Networks}, 
  title={Collective behavior of a small-world recurrent neural system with scale-free distribution}, 
  year={2007},
  volume={18},
  number={5},
  pages={1364--1375},
  keywords={Neurons;Nonlinear dynamical systems;Reservoirs;Spine;Complex networks;Laser modes;Biological system modeling;Power system modeling;Large-scale systems;Computer architecture;Echo state network (ESN);local preferential attachments;recurrent neural networks (RNNs);scale-free;small world;time-series prediction},
  doi={10.1109/TNN.2007.894082}
}

@ARTICLE{jarvis_2010,
  AUTHOR={Jarvis, Sarah  and Rotter, Stefan  and Egert, Ulrich },
  TITLE={Extending Stability Through Hierarchical Clusters in Echo State Networks},
  JOURNAL={Frontiers in Neuroinformatics},
  VOLUME={Volume 4 - 2010},
  YEAR={2010},
  URL={https://www.frontiersin.org/journals/neuroinformatics/articles/10.3389/fninf.2010.00011},
  DOI={10.3389/fninf.2010.00011},
  ISSN={1662-5196},
  ABSTRACT={<p>Echo State Networks (ESN) are reservoir networks that satisfy well-established criteria for stability when constructed as feedforward networks. Recent evidence suggests that stability criteria are altered in the presence of reservoir substructures, such as clusters. Understanding how the reservoir architecture affects stability is thus important for the appropriate design of any ESN. To quantitatively determine the influence of the most relevant network parameters, we analyzed the impact of reservoir substructures on stability in hierarchically clustered ESNs, as they allow a smooth transition from highly structured to increasingly homogeneous reservoirs. Previous studies used the largest eigenvalue of the reservoir connectivity matrix (spectral radius) as a predictor for stable network dynamics. Here, we evaluate the impact of clusters, hierarchy and intercluster connectivity on the predictive power of the spectral radius for stability. Both hierarchy and low relative cluster sizes extend the range of spectral radius values, leading to stable networks, while increasing intercluster connectivity decreased maximal spectral radius.</p>}
}


@ARTICLE{malik_2017,
  author={Malik, Zeeshan Khawar and Hussain, Amir and Wu, Qingming Jonathan},
  journal={IEEE Transactions on Cybernetics}, 
  title={Multilayered echo state machine: A novel architecture and algorithm}, 
  year={2017},
  volume={47},
  number={4},
  pages={946--959},
  keywords={Reservoirs;Neurons;Biological neural networks;Training;Recurrent neural networks;Standards;Cybernetics;Learning;multiple layer network and time series neural network;neural network},
  doi={10.1109/TCYB.2016.2533545}
}


@article{gallicchio_and_micheli_2017,
  title = {Deep reservoir computing: A critical experimental analysis},
  journal = {Neurocomputing},
  volume = {268},
  pages={87--99},
  year = {2017},
  note = {Advances in artificial neural networks, machine learning and computational intelligence},
  issn = {0925-2312},
  doi = {https://doi.org/10.1016/j.neucom.2016.12.089},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231217307567},
  author = {Claudio Gallicchio and Alessio Micheli and Luca Pedrelli},
  keywords = {Reservoir computing, Echo State Networks, Deep Learning, Deep neural networks, Recurrent neural networks, Multiple time-scale dynamics},
  abstract = {In this paper, we propose an empirical analysis of deep recurrent neural network (RNN) architectures with stacked layers. The main aim is to address some fundamental open research issues on the significance of creating deep layered architectures in RNN and to characterize the inherent hierarchical representation of time in such models, especially for efficient implementations. In particular, the analysis aims at the study and proposal of approaches to develop and enhance hierarchical dynamics in deep architectures within the efficient Reservoir Computing (RC) framework for RNN modeling. The effect of a deep layered organization of RC models is investigated in terms of both occurrence of multiple time-scale and increasing of richness of the dynamics. It turns out that a deep layering of recurrent models allows an effective diversification of temporal representations in the layers of the hierarchy, by amplifying the effects of the factors influencing the time-scales and the richness of the dynamics, measured as the entropy of recurrent units activations. The advantages of the proposed approach are also highlighted by measuring the increment of the short-term memory capacity of the RC models.}
}


@ARTICLE{long_2020,
  author={Long, Jianyu and Zhang, Shaohui and Li, Chuan},
  journal={IEEE Transactions on Industrial Informatics}, 
  title={Evolving deep echo state networks for intelligent fault diagnosis}, 
  year={2020},
  volume={16},
  number={7},
  pages={4928--4937},
  keywords={Optimization;Reservoirs;Fault diagnosis;Evolutionary computation;Encoding;Training;Informatics;Classification;echo state network (ESN);evolutionary algorithm;evolving neural network;fault diagnosis;neuroevolution},
  doi={10.1109/TII.2019.2938884}
}

@article{ma_2017,
  author       = {Qianli Ma and
                  Lifeng Shen and
                  Garrison W. Cottrell},
  title        = {Deep-esn: {a} Multiple Projection-encoding Hierarchical Reservoir
                  Computing Framework},
  journal      = {CoRR},
  volume       = {abs/1711.05255},
  year         = {2017},
  url          = {http://arxiv.org/abs/1711.05255},
  eprinttype    = {arXiv},
  eprint       = {1711.05255},
  timestamp    = {Mon, 04 Dec 2023 21:30:01 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1711-05255.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{carmichael_2018,
  author       = {Zachariah Carmichael and
                  Humza Syed and
                  Stuart Burtner and
                  Dhireesha Kudithipudi},
  title        = {Mod-deepesn: modular deep echo state network},
  journal      = {CoRR},
  volume       = {abs/1808.00523},
  year         = {2018},
  url          = {http://arxiv.org/abs/1808.00523},
  eprinttype    = {arXiv},
  eprint       = {1808.00523},
  timestamp    = {Sun, 02 Sep 2018 15:01:54 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1808-00523.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{li_2022,
  title = {Growing deep echo state network with supervised learning for time series prediction},
  journal = {Applied Soft Computing},
  volume = {128},
  pages = {109454},
  year = {2022},
  issn = {1568-4946},
  doi = {https://doi.org/10.1016/j.asoc.2022.109454},
  url = {https://www.sciencedirect.com/science/article/pii/S1568494622005683},
  author = {Ying Li and Fanjun Li},
  keywords = {Recurrent neural network, Reservoir computing, Echo state network, Deep learning, Supervised learning},
  abstract = {Multilayer echo state networks (ESNs) are powerful on learning hierarchical temporal representation. However, how to determine the depth of multilayer ESNs is still an open issue. In this paper, we propose a novel approach to automatically determine the depth of a multilayer ESN, named growing deep ESN (GD-ESN). First, an incremental hierarchical structure is proposed, where the recurrent layers and the pre-trained feedforward layers are alternately added to the network one by one. Then, a control scheme is designed for the growth of the network based on the newly defined averaged mutual information and the full rank criterion. Finally, the proposed GD-ESN is evaluated on both benchmark datasets and real-world applications. The experimental results show the effectiveness of the proposed method.}
}

@InProceedings{wcislo_2021,
  author="Wcis{\l}o, Robert
  and Czech, Wojciech",
  editor="Paszynski, Maciej
  and Kranzlm{\"u}ller, Dieter
  and Krzhizhanovskaya, Valeria V.
  and Dongarra, Jack J.
  and Sloot, Peter M. A.",
  title={Grouped multi-layer echo state networks with self-normalizing activations},
  booktitle="Computational Science -- ICCS 2021",
  year="2021",
  publisher="Springer International Publishing",
  address="Cham",
  pages="90--97",
  abstract="We study prediction performance of Echo State Networks with multiple reservoirs built based on stacking and grouping. Grouping allows for developing independent subreservoir dynamics, which improves linear separability on readout layer. At the same time, stacking enables to capture multiple time-scales of an input signal by the hierarchy of non-linear mappings. Combining those two effects, together with a proper selection of model hyperparameters can boost ESN capabilities for benchmark time-series such as Mackey Glass System. Different strategies for determining subreservoir structure are compared along with the influence of activation function. In particular, we show that recently proposed non-linear self-normalizing activation function together with grouped deep reservoirs provide superior prediction performance on artificial and real-world datasets. Moreover, comparing to standard tangent hyperbolic models, the new models built using self-normalizing activation function are more feasible in terms of hyperparameter selection.",
  isbn="978-3-030-77961-0"
}


@Article{lun_2023,
  AUTHOR = {Lun, Shuxian and Sun, Zhenduo and Li, Ming and Wang, Lei},
  TITLE = {Multiple-Reservoir Hierarchical Echo State Network},
  JOURNAL = {Mathematics},
  VOLUME = {11},
  YEAR = {2023},
  NUMBER = {18},
  ARTICLE-NUMBER = {3961},
  URL = {https://www.mdpi.com/2227-7390/11/18/3961},
  ISSN = {2227-7390},
  ABSTRACT = {Leaky Integrator Echo State Network (Leaky-ESN) is a useful training method for handling time series prediction problems. However, the singular coupling of all neurons in the reservoir makes Leaky-ESN less effective for sophisticated learning tasks. In this paper, we propose a new improvement to the Leaky-ESN model called the Multiple-Reservoir Hierarchical Echo State Network (MH-ESN). By introducing a new mechanism for constructing the reservoir, the efficiency of the network in handling training tasks is improved. The hierarchical structure is used in the process of constructing the reservoir mechanism of MH-ESN. The MH-ESN consists of multiple layers, each comprising a multi-reservoir echo state network model. The sub-reservoirs within each layer are linked via principal neurons, which mimics the functioning of a biological neural network. As a result, the coupling among neurons in the reservoir is decreased, and the internal dynamics of the reservoir are improved. Based on the analysis results, the MH-ESN exhibits significantly better prediction accuracy than Leaky-ESN for complex time series prediction.},
  DOI = {10.3390/math11183961}
}

@article{li_tanaka_2021,
  title = {Multi-reservoir echo state networks with sequence resampling for nonlinear time-series prediction},
  journal = {Neurocomputing},
  volume = {467},
  pages={115--129},
  year = {2022},
  issn = {0925-2312},
  doi = {https://doi.org/10.1016/j.neucom.2021.08.122},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231221013333},
  author = {Ziqiang Li and Gouhei Tanaka},
  keywords = {Nonlinear time-series prediction, Reservoir computing, Echo state network, Sequence resampling},
  abstract = {In this paper, we consider various schemes of sequence resampling in reservoir computing models for nonlinear time series prediction. These schemes can enrich the features used for training the readout part with batch learning and lead to better prediction performance. To implement these schemes, first, we introduce a modular approach for constructing multi-reservoir ESN models by assembling encoding and decoding modules. The encoding module is composed of a resampling unit, a group-wise reservoir unit, and a collection unit, for extracting various features from a sequence. The decoding module is a linear regressor which is trainable to produce desired outputs. Then, we propose three novel multi-reservoir ESN models, DeepESN with Every-layer Sequence Resampling (DeepESN-ESR), DeepESN with Last-layer Sequence Resampling (DeepESN-LSR), and GroupedESN with Input-layer Sequence Resampling (GroupedESN-ISR). These three models provide demonstrations of sequence resampling on multi-reservoir ESN models. Numerical results on five challenging nonlinear time-series prediction tasks show that the proposed models outperform some state-of-the-art multi-reservoir ESN models. An evaluation of computational time shows that our proposed three models require less computational cost for learning than many existing multi-reservoir ESN models in practice. Moreover, a comprehensive comparative analysis reveals that our proposed models are able to memorize longer temporal information and generate richer dynamics from the reservoir states than some existing models. The proposed schemes for extracting various features hidden in a sequence of reservoir states can be widely leveraged in other reservoir computing systems for improving their performance in nonlinear time series prediction.}
}

@inproceedings{dale_2018,
  author = {Dale, Matthew},
  title = {Neuroevolution of hierarchical reservoir computers},
  year = {2018},
  isbn = {9781450356183},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3205455.3205520},
  doi = {10.1145/3205455.3205520},
  abstract = {Reservoir Computers such as Echo State Networks (ESN) represent an alternative recurrent neural network model that provides fast training and state-of-the-art performances for supervised learning problems. Classic ESNs suffer from two limitations; hyperparameter selection and learning of multiple temporal and spatial scales. To learn multiple scales, hierarchies are proposed, and to overcome manual tuning, optimisation is used. However, the autonomous design of hierarchies and optimisation of multi-reservoir systems has not yet been demonstrated. In this work, an evolvable architecture is proposed called Reservoir-of-Reservoirs (RoR) where sub-networks of neurons (ESNs) are interconnected to form the reservoir. The design of each sub-network, hyperparameters, global connectivity and its hierarchical structure are evolved using a genetic algorithm (GA) called the Microbial GA.To evaluate the RoR and microbial GA, single networks and other hierarchical ESNs are evolved. The results show the microbial GA can dramatically increase the performance of single networks compared to other optimisation techniques. The evolutionary process also leads to competitive results with RoRs and other hierarchical ESNs, despite having fewer connections than a single network. In the final section, it is revealed that the RoR architecture may learn generalised features other architectures cannot, offering improvements in network generalisation to other tasks.},
  booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
  pages={410--417},
  numpages = {8},
  keywords = {echo state networks, microbial GA, neuroevolution, reservoir computing},
  location = {Kyoto, Japan},
  series = {GECCO '18}
  }

@inproceedings{argentieri_2022,
  title={Input routed echo state networks.},
  author={Argentieri, Luca and Gallicchio, Claudio and Micheli, Alessio and others},
  booktitle={ESANN 2022 Proceedings - 30th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning},
  pages={405--410},
  year={2022}
}

@article{ma_chen_2013,
  title = {Modular state space of echo state network},
  journal = {Neurocomputing},
  volume = {122},
  pages={406--417},
  year = {2013},
  note = {Advances in cognitive and ubiquitous computing},
  issn = {0925-2312},
  doi = {https://doi.org/10.1016/j.neucom.2013.06.012},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231213006206},
  author = {Qian-Li Ma and Wei-Biao Chen},
  keywords = {Echo state network (ESN), Modularity, Predictive modeling, Time series},
  abstract = {Echo state network (ESN) mainly consists of a reservoir with a large number of neurons that are randomly connected and a linear readout (output) that is easily adapted. From this point, the reservoir will reconstruct the input signals in the high-dimensional state space. In this paper, modular state space of echo state network (MSSESN) is proposed. First, the state space is divided into several subspaces and each of which is called “a module”. And then, linear readout of ESN is replaced by piecewise output function which maps each module states individually to the actual output. Furthermore, unlike iterative prediction in ESN, the feedback connections from the output neuron to the reservoir are eliminated, which establishes a direct relationship between the reservoir and output. Finally, the final results can be obtained by assembling the outputs of each module. Different from previous reservoir computing methods, MSSESN takes advantage of the modularity and reservoir mechanisms. It is theoretically analyzed and tested by the benchmark prediction of Mackey–Glass and Lorenz time series. The results have proven the effectiveness of this methodology.}
}

@article {laan_vicente_2015,
	author = {Laan, Andres and Vicente, Raul},
	title = {Echo state networks with multiple readout modules},
	elocation-id = {017558},
	year = {2017},
	doi = {10.1101/017558},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {We propose a new readout architecture for echo state networks where multiple linear readout modules are activated at distinct time points to varying degrees by a separate controller module. The controller module, like the reservoir of the echo state network, can be initialized randomly. All linear readout modules are trained through simple linear regression, which is the only adaptive step in the modified algorithm. The resulting architecture provides modest improvements on a variety of time series processing tasks (between 5 to 50\% in performance metric depending on the task studied). The novel architecture is guaranteed to perform at least as accurately as a conventional linear readout. It can be utilized as a general purpose readout method when augmentations to performance relative to the standard method is needed.PACS numbers: 05.45.Tp, 07.05.Mh},
	URL = {https://www.biorxiv.org/content/early/2017/03/02/017558},
	eprint = {https://www.biorxiv.org/content/early/2017/03/02/017558.full.pdf},
	journal = {bioRxiv}
}


@article{bandt_pompe_2002,
  title = {Permutation entropy: A natural complexity measure for time series},
  author = {Bandt, Christoph and Pompe, Bernd},
  journal = {Phys. Rev. Lett.},
  volume = {88},
  issue = {17},
  pages = {174102},
  numpages = {4},
  year = {2002},
  month = {Apr},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.88.174102},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.88.174102}
}


@article{zanin_2023,
    author = {Zanin, Massimiliano},
    title = {Continuous ordinal patterns: creating a bridge between ordinal analysis and deep learning},
    journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
    volume = {33},
    number = {3},
    pages = {033114},
    year = {2023},
    month = {03},
    abstract = {We introduce a generalization of the celebrated ordinal pattern approach for the analysis of time series, in which these are evaluated in terms of their distance to ordinal patterns defined in a continuous way. This allows us to naturally incorporate information about the local amplitude of the data and to optimize the ordinal pattern(s) to the problem under study. This last element represents a novel bridge between standard ordinal analysis and deep learning, allowing the achievement of results comparable to the latter in real-world classification problems while also retaining the conceptual simplicity, computational efficiency, and easy interpretability of the former. We test this through the use of synthetic time series, generated by standard chaotic maps and dynamical models, data sets representing brain activity in health and schizophrenia, and the dynamics of delays in the European air transport system. We further show how the continuous ordinal patterns can be used to assess other aspects of the dynamics, like time irreversibility.},
    issn = {1054-1500},
    doi = {10.1063/5.0136492},
    url = {https://doi.org/10.1063/5.0136492},
    eprint = {https://pubs.aip.org/aip/cha/article-pdf/doi/10.1063/5.0136492/16788568/033114\_1\_online.pdf},
}

@article{amigo_2007,
  doi = {10.1209/0295-5075/79/50001},
  url = {https://dx.doi.org/10.1209/0295-5075/79/50001},
  year = {2007},
  month = {jul},
  publisher = {},
  volume = {79},
  number = {5},
  pages = {50001},
  author = {Amigó, J. M. and Zambrano, S. and Sanjuán, M. A. F.},
  title = {True and false forbidden patterns in deterministic and random dynamics},
  journal = {Europhysics Letters},
  abstract = {In this letter we discuss some properties of order patterns both in deterministic and random orbit generation. As it turns out, the orbits of one-dimensional maps have always forbidden patterns, i.e., order patterns that cannot occur, in contrast with random time series, in which any order pattern appears with probability one. However, finite random sequences may exhibit “false” forbidden patterns with non-vanishing probability. In this case, forbidden patterns decay with the sequence length, thus unveiling the random nature of the sequence. Last but not least, true forbidden patterns are robust against noise and disintegrate with a rate that depends on the noise level. These properties can be embodied in a simple method to distinguish deterministic, finite time series with very high levels of observational noise, from random ones. We present numerical evidence for white noise.}
}


@article{zanin_2013,
    author = {Zanin, Massimiliano},
    title = {Forbidden patterns in financial time series},
    journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
    volume = {18},
    number = {1},
    pages = {013119},
    year = {2008},
    month = {03},
    abstract = {The existence of forbidden patterns, i.e., certain missing sequences in a given time series, is a recently proposed instrument of potential application in the study of time series. Forbidden patterns are related to the permutation entropy, which has the basic properties of classic chaos indicators, such as Lyapunov exponent or Kolmogorov entropy, thus allowing to separate deterministic (usually chaotic) from random series; however, it requires fewer values of the series to be calculated, and it is suitable for using with small datasets. In this paper, the appearance of forbidden patterns is studied in different economical indicators such as stock indices (Dow Jones Industrial Average and Nasdaq Composite), NYSE stocks (IBM and Boeing), and others (ten year Bond interest rate), to find evidence of deterministic behavior in their evolutions. Moreover, the rate of appearance of the forbidden patterns is calculated, and some considerations about the underlying dynamics are suggested.},
    issn = {1054-1500},
    doi = {10.1063/1.2841197},
    url = {https://doi.org/10.1063/1.2841197},
    eprint = {https://pubs.aip.org/aip/cha/article-pdf/doi/10.1063/1.2841197/14600814/013119\_1\_online.pdf},
}


@article{liu_2020,
  title = {Representation based on ordinal patterns for seizure detection in EEG signals},
  journal = {Computers in Biology and Medicine},
  volume = {126},
  pages = {104033},
  year = {2020},
  issn = {0010-4825},
  doi = {https://doi.org/10.1016/j.compbiomed.2020.104033},
  url = {https://www.sciencedirect.com/science/article/pii/S0010482520303644},
  author = {Yunxiao Liu and Youfang Lin and Ziyu Jia and Yan Ma and Jing Wang},
  keywords = {Ordinal pattern, Classification, Cooccurrence, Seizure detection, EEG},
  abstract = {EEG signals carry rich information about brain activity and play an important role in the diagnosis and recognition of epilepsy. Numerous algorithms using EEG signals to detect seizures have been developed in recent decades. However, most of them require well-designed features that highly depend on domain-specific knowledge and algorithm expertise. In this study, we introduce the unigram ordinal pattern (UniOP) and bigram ordinal pattern (BiOP) representations to capture the different underlying dynamics of time series, which only assumes that time series derived from different dynamics can be characterized by repeated ordinal patterns. Specifically, we first transform each subsequence in a time series into the corresponding ordinal pattern in terms of the ranking of values and consider the distribution of ordinal patterns of all subsequences as the UniOP representation. Furthermore, we consider the distribution of the cooccurrence of ordinal patterns as the BiOP representation to characterize the contextual information for each ordinal pattern. We then combine the proposed representations with the nearest neighbor algorithm to evaluate its effectiveness on three publicly available seizure datasets. The results on the Bonn EEG dataset demonstrate that this method provides more than 90% accuracy, sensitivity, and specificity in most cases and outperforms several state-of-the-art methods, which proves its ability to capture the key information of the underlying dynamics of EEG time series at healthy, seizure-free, and seizure states. The results on the second dataset are comparable with the state-of-the-art method, showing the good generalization ability of the proposed method. All performance metrics on the third dataset are approximately 89%, which demonstrates that the proposed representations are suitable for large-scale datasets.}
}


@inbook{keller_2007,
  title = {Ordinal analysis of EEG time series},
  abstract = "Ordinal time series analysis is a new approach to the qualitative investigation of long and complex time series. The idea behind it is to transform a given time series into a series of ordinal patterns each describing the order relations between the present and a fixed number of equidistant past values at a given time. Here we consider ordinal pattern distributions and some measures derived from them in order to detect differences between EEG data.",
  author = "Karsten Keller and Heinz Lauffer and Mathieu Sinn",
  year = "2009",
  month = dec,
  day = "1",
  language = "English",
  isbn = "9781604568417",
  pages = "239--249",
  booktitle = "Chaos and Complexity",
  publisher = "Nova Science Publishers, Inc.",
}

@article{boaretto_2021,
  title={Discriminating chaotic and stochastic time series using permutation entropy and artificial neural networks},
  author={Boaretto, BRR and Masoller, C},
  journal={Scientific reports},
  volume={11},
  number={1},
  pages={15789},
  year={2021},
  publisher={Nature Publishing Group UK London}
}


@inproceedings{sinn_2012,
  author = {Sinn, Mathieu and Ghodsi, Ali and Keller, Karsten},
  title = {Detecting change-points in time series by maximum mean discrepancy of ordinal pattern distributions},
  year = {2012},
  isbn = {9780974903989},
  publisher = {AUAI Press},
  address = {Arlington, Virginia, USA},
  abstract = {As a new method for detecting change-points in high-resolution time series, we apply Maximum Mean Discrepancy to the distributions of ordinal patterns in different parts of a time series. The main advantage of this approach is its computational simplicity and robustness with respect to (non-linear) monotonic transformations, which makes it particularly well-suited for the analysis of long biophysical time series where the exact calibration of measurement devices is unknown or varies with time. We establish consistency of the method and evaluate its performance in simulation studies. Furthermore, we demonstrate the application to the analysis of electroencephalography (EEG) and electrocardiography (ECG) recordings.},
  booktitle = {Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence},
  pages={786--794},
  numpages = {9},
  location = {Catalina Island, CA},
  series = {UAI'12}
}

@InProceedings{thorne_2022,
  author="Thorne, Braden
  and J{\"u}ngling, Thomas
  and Small, Michael
  and Corr{\^e}a, D{\'e}bora
  and Zaitouny, Ayham",
  editor="Aziz, Haris
  and Corr{\^e}a, D{\'e}bora
  and French, Tim",
  title={A novel approach to time series complexity via reservoir computing},
  booktitle="AI 2022: Advances in Artificial Intelligence",
  year="2022",
  publisher="Springer International Publishing",
  address="Cham",
  pages="442--455",
  abstract="When working with time series, it is often beneficial to have an idea as to how complex the signal is. Periodic, chaotic and random signals (from least to most complex) may each be approached in different ways, and knowing when a signal can be identified as belonging to one of these categories can reveal a lot about the underlying system. In the field of time series analysis, permutation entropy has emerged as one of the premier measures of time series complexity due to its ability to be calculated from data alone. We propose an alternative method for calculating complexity based on the machine learning paradigm of reservoir computing, and how the outputs of these neural networks capture similar information regarding signal complexity. We observe similar behaviour in our proposed measure to both the Lyapunov exponent and permutation entropy for well known dynamical systems. Additionally, we assess the dependence of our measure on key hyperparameters of the model, drawing conclusions about the invariance of the measure and possible implications on informing network structure.",
  isbn="978-3-031-22695-3"
}




@Article{sun_2022,
  AUTHOR = {Sun, Xiaochuan and Hao, Mingxiang and Wang, Yutong and Wang, Yu and Li, Zhigang and Li, Yingqi},
  TITLE = {Reservoir Dynamic Interpretability for Time Series Prediction: A Permutation Entropy View},
  JOURNAL = {Entropy},
  VOLUME = {24},
  YEAR = {2022},
  NUMBER = {12},
  ARTICLE-NUMBER = {1709},
  URL = {https://www.mdpi.com/1099-4300/24/12/1709},
  PubMedID = {36554114},
  ISSN = {1099-4300},
  ABSTRACT = {An echo state network (ESN) is an efficient recurrent neural network (RNN) that is widely used in time series prediction tasks due to its simplicity and low training cost. However, the “black-box” nature of reservoirs hinders the development of ESN. Although a large number of studies have concentrated on reservoir interpretability, the perspective of reservoir modeling is relatively single, and the relationship between reservoir richness and reservoir projection capacity has not been effectively established. To tackle this problem, a novel reservoir interpretability framework based on permutation entropy (PE) theory is proposed in this paper. In structure, this framework consists of reservoir state extraction, PE modeling, and PE analysis. Based on these, the instantaneous reservoir states and neuronal time-varying states are extracted, which are followed by phase space reconstruction, sorting, and entropy calculation. Firstly, the obtained instantaneous state entropy (ISE) and global state entropy (GSE) can measure reservoir richness for interpreting good reservoir projection capacity. On the other hand, the multiscale complexity–entropy analysis of global and neuron-level reservoir states is performed to reveal more detailed dynamics. Finally, the relationships between ESN performance and reservoir dynamic are investigated via Pearson correlation, considering different prediction steps and time scales. Experimental evaluations on several benchmarks and real-world datasets demonstrate the effectiveness and superiority of the proposed reservoir interpretability framework.},
  DOI = {10.3390/e24121709}
}



@article{leyva_2022,
  doi = {10.1209/0295-5075/ac6a72},
  url = {https://dx.doi.org/10.1209/0295-5075/ac6a72},
  year = {2022},
  month = {may},
  publisher = {EDP Sciences, IOP Publishing and Società Italiana di Fisica},
  volume = {138},
  number = {3},
  pages = {31001},
  author = {Leyva, Inmaculada and Martínez, Johann H. and Masoller, Cristina and Rosso, Osvaldo A. and Zanin, Massimiliano},
  title = {20 years of ordinal patterns: perspectives and challenges},
  journal = {Europhysics Letters},
  abstract = {In 2002, in a seminal article, Bandt and Pompe proposed a new methodology for the analysis of complex time series, now known as Ordinal Analysis. The ordinal methodology is based on the computation of symbols (known as ordinal patters) which are defined in terms of the temporal ordering of data points in a time series, and whose probabilities are known as ordinal probabilities. With the ordinal probabilities the Shannon entropy can be calculated, which is the permutation entropy. Since it was proposed, the ordinal method has found applications in fields as diverse as biomedicine and climatology. However, some properties of ordinal probabilities are still not fully understood, and how to combine the ordinal approach of feature extraction with machine learning techniques for model identification, time series classification or forecasting, remains a challenge. The objective of this perspective article is to present some recent advances and to discuss some open problems.}
}



@ARTICLE{qiao_2016,
  author={Qiao, Junfei and Li, Fanjun and Han, Honggui and Li, Wenjing},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Growing echo-state network with multiple subreservoirs}, 
  year={2017},
  volume={28},
  number={2},
  pages={391--404},
  keywords={Reservoirs;Complexity theory;Topology;Training;Recurrent neural networks;Network topology;Benchmark testing;Echo-state network (ESN);growing echo-state network (GESN);recurrent neural network (RNN);reservoir;time-series prediction},
  doi={10.1109/TNNLS.2016.2514275}}


@article { lorenz,
  author = "Edward N.  Lorenz",
  title = {Deterministic nonperiodic flow},
  journal = "Journal of Atmospheric Sciences",
  year = "1963",
  publisher = "American Meteorological Society",
  address = "Boston MA, USA",
  volume = "20",
  number = "2",
  doi = "10.1175/1520-0469(1963)020<0130:DNF>2.0.CO;2",
  pages=      "130 - 141",
  url = "https://journals.ametsoc.org/view/journals/atsc/20/2/1520-0469_1963_020_0130_dnf_2_0_co_2.xml"
}

@article{rossler,
title = {An equation for continuous chaos},
journal = {Physics Letters A},
volume = {57},
number = {5},
pages={397--398},
year = {1976},
issn = {0375-9601},
doi = {https://doi.org/10.1016/0375-9601(76)90101-8},
url = {https://www.sciencedirect.com/science/article/pii/0375960176901018},
author = {O.E. Rössler},
abstract = {A prototype equation to the Lorenz model of turbulence contains just one (second-order) nonlinearity in one variable. The flow in state space allows for a “folded” Poincaré map (horseshoe map). Many more natural and artificial systems are governed by this type of equation.}
}

@article{mackey_glass,
author = {Michael C. Mackey  and Leon Glass },
title = {Oscillation and chaos in physiological control systems},
journal = {Science},
volume = {197},
number = {4300},
pages={287--289},
year = {1977},
doi = {10.1126/science.267326},
URL = {https://www.science.org/doi/abs/10.1126/science.267326},
eprint = {https://www.science.org/doi/pdf/10.1126/science.267326},
abstract = {First-order nonlinear differential-delay equations describing physiological control systems are studied. The equations display a broad diversity of dynamical behavior including limit cycle oscillations, with a variety of wave forms, and apparently aperiodic or "chaotic" solutions. These results are discussed in relation to dynamical respiratory and hematopoietic diseases.}}

@article{masoudnia_2014,
  title={Mixture of experts: a literature survey},
  author={Masoudnia, Saeed and Ebrahimpour, Reza},
  journal={Artificial Intelligence Review},
  volume={42},
  pages={275--293},
  year={2014},
  publisher={Springer}
}


@ARTICLE{yuksel_2012,
  author={Yuksel, Seniha Esen and Wilson, Joseph N. and Gader, Paul D.},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Twenty years of mixture of experts}, 
  year={2012},
  volume={23},
  number={8},
  pages={1177--1193},
  keywords={Bayesian methods;Hidden Markov models;Regression analysis;Support vector machines;Data models;Decision trees;Gaussian processes;Applications;Bayesian;classification;comparison;hierarchical mixture of experts (HME);mixture of Gaussian process experts;regression;statistical properties;survey;variational},
  doi={10.1109/TNNLS.2012.2200299}}



@ARTICLE{hochreiter_1997,
  author={Hochreiter, Sepp and Schmidhuber, Jürgen},
  journal={Neural Computation}, 
  title={Long short-term memory}, 
  year={1997},
  volume={9},
  number={8},
  pages={1735--1780},
  keywords={},
  doi={10.1162/neco.1997.9.8.1735}}



@article{riedl_2013,
  title={Practical considerations of permutation entropy: A tutorial review},
  author={Riedl, Maik and M{\"u}ller, Andreas and Wessel, Niels},
  journal={The European Physical Journal Special Topics},
  volume={222},
  number={2},
  pages={249--262},
  year={2013},
  publisher={Springer}
}

@book{kantz_2003,
  author = {Kantz, Holger and Schreiber, Thomas},
  title = {Nonlinear time series analysis},
  year = {2003},
  isbn = {0521529026},
  publisher = {Cambridge University Press},
  address = {USA}
}
